{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Sum-base\n",
    "Draft version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import nltk\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "import logging\n",
    "import os\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikihow = pd.read_csv(\"data/clean_wikihow.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sell yourself first</td>\n",
       "      <td>anything else stop sum artist think translate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>read the classics before 1600</td>\n",
       "      <td>reading classics first thing well read want bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>join online artist communities</td>\n",
       "      <td>depending scale intend sell art pieces may wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>make yourself public</td>\n",
       "      <td>get best advertising publish example pieces ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blog about your artwork</td>\n",
       "      <td>given hundreds free blogging websites lot choi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           summary  \\\n",
       "0             sell yourself first    \n",
       "1   read the classics before 1600    \n",
       "2  join online artist communities    \n",
       "3            make yourself public    \n",
       "4         blog about your artwork    \n",
       "\n",
       "                                                text  \n",
       "0  anything else stop sum artist think translate ...  \n",
       "1  reading classics first thing well read want bu...  \n",
       "2  depending scale intend sell art pieces may wan...  \n",
       "3  get best advertising publish example pieces ar...  \n",
       "4  given hundreds free blogging websites lot choi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikihow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure the settings are the same as your training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1212030 (1212030, 2)\n"
     ]
    }
   ],
   "source": [
    "summaries = wikihow['summary'].tolist()\n",
    "texts = wikihow['text'].tolist()\n",
    "print(len(summaries), wikihow.shape)\n",
    "\n",
    "#################### CONFIG #############\n",
    "# dataset range\n",
    "si, ei = 0, 120000 #0, 25000\n",
    "# Subset the data for training\n",
    "start = 0 # 0\n",
    "end = start + 100000 # 20000\n",
    "\n",
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 128 # 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75\n",
    "###########################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in str(sentence).split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 336845\n"
     ]
    }
   ],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary\n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, summaries)\n",
    "count_words(word_counts, texts)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 516783\n"
     ]
    }
   ],
   "source": [
    "# Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better \n",
    "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
    "embeddings_index = {}\n",
    "with open('numberbatch-en-19.08.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 10507\n",
      "Percent of words that are missing from vocabulary: 3.1199999999999997%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 10\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 336845\n",
      "Number of words we will use: 102501\n",
      "Percent of words we will use: 30.43%\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocab that we will use to words that appear â‰¥ threshold or are in GloVe\n",
    "\n",
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jbu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}\n",
    "\n",
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/best_model.ckpt\n",
      "\n",
      "==================== Example 3 ====================\n",
      "Original Text:\n",
      "get best advertising publish example pieces around web show demonstrate style sure add watermark digital version protect artwork art thieves spend little time researching online ways artists promoting works like promotional strategies want avoid type research give lot ideas also alert potential pitfalls ways promote artwork bookmark sites artists selling online really inspire come back regularly see evolving succeeding seize power twitter facebook increase people knowledge tweet updates new paintings thoughts art news items art general facebook place photos artwork digitally watermarked photos receiving awards information art artwork general perhaps even critiques artwork\n",
      "\n",
      "Original Summary:\n",
      "make yourself public \n",
      "\n",
      "Input:\n",
      "Word Ids: [109, 127, 17443, 184, 1004, 3852, 534, 9342, 1530, 7002, 183, 26, 268, 21254, 2937, 6020, 806, 17, 30, 23979, 549, 520, 322, 8606, 9, 539, 14962, 10398, 758, 115, 13920, 1638, 108, 266, 2741, 92, 523, 44, 784, 1366, 2025, 446, 5658, 539, 3289, 17, 14535, 2478, 14962, 11605, 9, 4569, 3612, 380, 116, 1118, 428, 8788, 9600, 16786, 875, 16196, 5490, 1193, 113, 943, 21669, 3490, 203, 11122, 990, 30, 2265, 1301, 30, 2281, 5490, 874, 5496, 17, 21610, 44477, 5496, 3597, 18650, 142, 30, 17, 2281, 9266, 1002, 20114, 17]\n",
      "get best advertising publish example pieces around web show demonstrate style sure add watermark digital version protect artwork art thieves spend little time researching online ways artists promoting works like promotional strategies want avoid type research give lot ideas also alert potential pitfalls ways promote artwork bookmark sites artists selling online really inspire come back regularly see evolving succeeding seize power twitter facebook increase people knowledge tweet updates new paintings thoughts art news items art general facebook place photos artwork digitally watermarked photos receiving awards information art artwork general perhaps even critiques artwork\n",
      "\n",
      "Output Summary:\n",
      "Word Ids: [4, 4, 4769, 4, 4769, 53]\n",
      "the the gaseous the gaseous you\n",
      "\n",
      "==================== Example 4 ====================\n",
      "Original Text:\n",
      "given hundreds free blogging websites lot choice keeping blog importantly extremely useful keep updated regularly gives people something pretty look story follow reputation increases sales story grows blog pick lot hits search engines utilize keywords feature accurately use advantage sure name blog something simple memorable â€“ want people able find ease read start blog increase website traffic free assistance good way test keywords little cost using auction site play around words use title art sales well words used within body text keep changing words find sweet spot â€“ words really seem attract views\n",
      "\n",
      "Original Summary:\n",
      "blog about your artwork \n",
      "\n",
      "Input:\n",
      "Word Ids: [2672, 15506, 580, 20177, 7984, 44, 2162, 4224, 14, 5694, 1981, 5482, 283, 2839, 1118, 5571, 113, 569, 6110, 65, 90, 727, 15030, 1356, 10515, 90, 6260, 14, 126, 44, 10562, 1926, 22658, 1959, 11715, 5033, 8571, 97, 5575, 26, 50, 14, 569, 579, 2820, 4972, 108, 113, 1907, 137, 2578, 3, 222, 14, 1193, 463, 1257, 580, 3085, 23, 898, 1008, 11715, 520, 575, 383, 15227, 36, 841, 534, 3372, 97, 3029, 30, 10515, 232, 3372, 494, 1349, 891, 2367, 283, 264, 3372, 137, 4645, 771, 4972, 3372, 4569, 4226, 8559, 779]\n",
      "given hundreds free blogging websites lot choice keeping blog importantly extremely useful keep updated regularly gives people something pretty look story follow reputation increases sales story grows blog pick lot hits search engines utilize keywords feature accurately use advantage sure name blog something simple memorable â€“ want people able find ease read start blog increase website traffic free assistance good way test keywords little cost using auction site play around words use title art sales well words used within body text keep changing words find sweet spot â€“ words really seem attract views\n",
      "\n",
      "Output Summary:\n",
      "Word Ids: [247, 16, 887, 16, 850, 850, 47515]\n",
      "focus your spruce your charge charge stargazing\n",
      "\n",
      "==================== Example 5 ====================\n",
      "Original Text:\n",
      "could effective tool managed well every sale make every person sends email might interested get email addresses digital database ready mass emailing designated intervals month every week whenever start new series send nice grammatically correct friendly emails complete neatly set portfolio pictures recent work pdf work well purpose keeps radar past customers important really wonderful newsletter might get lucky might send friends well â€“ even eyes seeing work every thoughtful mailing list includes opt option threatened see good housekeeping retaining clients really want see items online hassle people interested\n",
      "\n",
      "Original Summary:\n",
      "create a mailing list \n",
      "\n",
      "Input:\n",
      "Word Ids: [2192, 2731, 1861, 6588, 232, 602, 9057, 12, 602, 948, 16527, 7531, 2398, 5883, 109, 7531, 18577, 2937, 3425, 1886, 3725, 20991, 4408, 3702, 2542, 602, 695, 1353, 222, 203, 7016, 2926, 3163, 20544, 1267, 560, 8917, 2336, 17623, 481, 12144, 24, 3903, 165, 16366, 165, 232, 3412, 2555, 18237, 897, 12388, 800, 4569, 6190, 15225, 2398, 109, 9824, 2398, 2926, 172, 232, 4972, 1002, 1306, 11126, 165, 602, 11329, 20, 21, 1658, 1551, 32, 5718, 428, 23, 23346, 24765, 12139, 4569, 108, 428, 1301, 9, 23284, 113, 5883]\n",
      "could effective tool managed well every sale make every person sends email might interested get email addresses digital database ready mass emailing designated intervals month every week whenever start new series send nice grammatically correct friendly emails complete neatly set portfolio pictures recent work pdf work well purpose keeps radar past customers important really wonderful newsletter might get lucky might send friends well â€“ even eyes seeing work every thoughtful mailing list includes opt option threatened see good housekeeping retaining clients really want see items online hassle people interested\n",
      "\n",
      "Output Summary:\n",
      "Word Ids: [4, 4, 857, 33, 16, 3018]\n",
      "the the triggers of your forms\n",
      "\n",
      "==================== Example 6 ====================\n",
      "Original Text:\n",
      "like say picture worth thousand words want impression potential customers good show detail right paint strokes paint type willing show potential buyers multiple shots work different light hung wall etc may way alleviate justifiable fear many people buying work art never seen person bear mind everyone good internet connection balance great photos ease downloading time talk web specialist sure photography tips relevance read take better product photographs free take catalog photos\n",
      "\n",
      "Original Summary:\n",
      "take good pictures \n",
      "\n",
      "Input:\n",
      "Word Ids: [115, 955, 2893, 5040, 14685, 3372, 108, 5094, 446, 12388, 23, 1530, 6000, 45, 281, 5371, 281, 2741, 150, 1530, 446, 19394, 639, 7792, 165, 1168, 1457, 20388, 2240, 799, 533, 898, 2237, 26007, 1625, 839, 113, 5292, 165, 30, 1379, 8480, 948, 8131, 803, 2989, 23, 198, 4746, 689, 2891, 5496, 2578, 23225, 322, 599, 9342, 2685, 26, 9886, 3743, 8584, 3, 22, 311, 1143, 2963, 580, 22, 10114, 5496]\n",
      "like say picture worth thousand words want impression potential customers good show detail right paint strokes paint type willing show potential buyers multiple shots work different light hung wall etc may way alleviate justifiable fear many people buying work art never seen person bear mind everyone good internet connection balance great photos ease downloading time talk web specialist sure photography tips relevance read take better product photographs free take catalog photos\n",
      "\n",
      "Output Summary:\n",
      "Word Ids: [4, 4, 1121, 53, 3280]\n",
      "the the feces you transducer\n",
      "\n",
      "==================== Example 7 ====================\n",
      "Original Text:\n",
      "licensing art way proving belongs necessary bookkeeping arts department help dispute originality ownership moral rights\n",
      "\n",
      "Original Summary:\n",
      "be sure to properly license your art \n",
      "\n",
      "Input:\n",
      "Word Ids: [8100, 30, 898, 27291, 13026, 732, 17613, 10355, 8761, 251, 11214, 20929, 8641, 13767, 2705]\n",
      "licensing art way proving belongs necessary bookkeeping arts department help dispute originality ownership moral rights\n",
      "\n",
      "Output Summary:\n",
      "Word Ids: [25, 1370, 33, 4, 4769, 47515, 47515]\n",
      "be ensure of the gaseous stargazing stargazing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Example 9 ====================\n",
      "Original Text:\n",
      "online art business needs built little little much putting together company indeed treat online art sales business â€“ try make name known least give people hint artist develop services grow gain reputation time goes may feel like long time beginning solid patient foundations set good future well maintaining positive attitude attend many relevant art shows show work shows juried display awards art part profiles website backgrounds\n",
      "\n",
      "Original Summary:\n",
      "expect this to be a gradual process and do not expect to sell a lot right away \n",
      "\n",
      "Input:\n",
      "Word Ids: [9, 30, 122, 514, 4089, 520, 520, 301, 2799, 843, 2842, 654, 1038, 9, 30, 10515, 122, 4972, 430, 12, 50, 2322, 314, 523, 113, 17459, 10, 180, 3427, 95, 1915, 15030, 322, 5410, 533, 475, 115, 1287, 322, 2184, 2969, 713, 17838, 481, 23, 722, 232, 1667, 498, 853, 1922, 839, 5041, 30, 107, 1530, 165, 107, 28932, 3483, 18650, 30, 1244, 12142, 463, 14686]\n",
      "online art business needs built little little much putting together company indeed treat online art sales business â€“ try make name known least give people hint artist develop services grow gain reputation time goes may feel like long time beginning solid patient foundations set good future well maintaining positive attitude attend many relevant art shows show work shows juried display awards art part profiles website backgrounds\n",
      "\n",
      "Output Summary:\n",
      "Word Ids: [4, 4, 4769, 4, 1121, 53]\n",
      "the the gaseous the feces you\n",
      "\n",
      "==================== Example 11 ====================\n",
      "Original Text:\n",
      "easy spend hours focused booking old short fat thin classic enough edgy enough focusing things get work instead focus makes good performer\n",
      "\n",
      "Original Summary:\n",
      "examine your strengths as a performer \n",
      "\n",
      "Input:\n",
      "Word Ids: [290, 549, 1131, 1969, 5985, 2825, 77, 1977, 3729, 11108, 1289, 21538, 1289, 8628, 166, 109, 165, 432, 247, 105, 23, 57]\n",
      "easy spend hours focused booking old short fat thin classic enough edgy enough focusing things get work instead focus makes good performer\n",
      "\n",
      "Output Summary:\n",
      "Word Ids: [25, 4, 23, 19990, 47515, 47515]\n",
      "be the good engineer stargazing stargazing\n",
      "\n",
      "==================== Example 13 ====================\n",
      "Original Text:\n",
      "enable recovering passwords order histories etc helps save personal information credit debit card information well ability print receipts tickets soon purchased\n",
      "\n",
      "Original Summary:\n",
      "create a ticketmaster account \n",
      "\n",
      "Input:\n",
      "Word Ids: [5032, 4352, 12506, 586, 7676, 799, 1495, 312, 182, 142, 9434, 16198, 501, 142, 232, 3037, 357, 8435, 149, 377, 7312]\n",
      "enable recovering passwords order histories etc helps save personal information credit debit card information well ability print receipts tickets soon purchased\n",
      "\n",
      "Output Summary:\n",
      "Word Ids: [25, 1488, 27, 16, 887, 47515]\n",
      "be ultraviolet to your spruce stargazing\n",
      "\n",
      "==================== Example 20 ====================\n",
      "Original Text:\n",
      "listen local radio broadcasts advertisements reference casinos area none area listen national radio broadcasts advertisements casinos areas note location mentioned advertisement involves casino locations mentioned note additional contact information website phone number use information find casinos\n",
      "\n",
      "Original Summary:\n",
      "listen to radio advertisements \n",
      "\n",
      "Input:\n",
      "Word Ids: [82, 244, 83, 15997, 84, 8510, 35652, 153, 3685, 153, 82, 5850, 83, 15997, 84, 35652, 1258, 2380, 240, 4899, 20004, 1747, 242, 6980, 4899, 2380, 995, 1450, 142, 463, 195, 1342, 97, 142, 137, 35652]\n",
      "listen local radio broadcasts advertisements reference casinos area none area listen national radio broadcasts advertisements casinos areas note location mentioned advertisement involves casino locations mentioned note additional contact information website phone number use information find casinos\n",
      "\n",
      "Output Summary:\n",
      "Word Ids: [4, 4, 857, 33, 4]\n",
      "the the triggers of the\n",
      "\n",
      "==================== Example 21 ====================\n",
      "Original Text:\n",
      "stage name way express want name signify think stage name might able channel performance persona\n",
      "\n",
      "Original Summary:\n",
      "choose a name that reflects your persona \n",
      "\n",
      "Input:\n",
      "Word Ids: [49, 50, 898, 5380, 108, 50, 6946, 725, 49, 50, 2398, 1907, 1930, 8740, 88]\n",
      "stage name way express want name signify think stage name might able channel performance persona\n",
      "\n",
      "Output Summary:\n",
      "Word Ids: [25, 1370, 4, 4769, 47515, 47515, 47515]\n",
      "be ensure the gaseous stargazing stargazing stargazing\n",
      "\n",
      "==================== Example 22 ====================\n",
      "Original Text:\n",
      "whatever stage name people likely want know decided call itâ€™s uninteresting story perhaps think making exciting story go name\n",
      "\n",
      "Original Summary:\n",
      "have a story behind your name \n",
      "\n",
      "Input:\n",
      "Word Ids: [574, 49, 50, 113, 2186, 108, 104, 7076, 702, 1177, 14998, 90, 9266, 725, 2980, 13622, 90, 154, 50]\n",
      "whatever stage name people likely want know decided call itâ€™s uninteresting story perhaps think making exciting story go name\n",
      "\n",
      "Output Summary:\n",
      "Word Ids: [25, 1370, 33, 4, 4769, 47515]\n",
      "be ensure of the gaseous stargazing\n",
      "\n",
      "==================== Example 23 ====================\n",
      "Original Text:\n",
      "look online name books learn meaning name youâ€™ve chosen learn history name nameâ€™s meaning history reflect want mean\n",
      "\n",
      "Original Summary:\n",
      "do research about your name \n",
      "\n",
      "Input:\n",
      "Word Ids: [65, 9, 50, 218, 176, 4732, 50, 5909, 9031, 176, 971, 50, 47081, 4732, 971, 2845, 108, 808]\n",
      "look online name books learn meaning name youâ€™ve chosen learn history name nameâ€™s meaning history reflect want mean\n",
      "\n",
      "Output Summary:\n",
      "Word Ids: [25, 19, 23, 19990, 47515]\n",
      "be a good engineer stargazing\n",
      "\n",
      "==================== Example 24 ====================\n",
      "Original Text:\n",
      "think people find name search engines like google use common words especially single words like trouble heart may hard fans find online\n",
      "\n",
      "Original Summary:\n",
      "choose a searchable name \n",
      "\n",
      "Input:\n",
      "Word Ids: [725, 113, 137, 50, 1926, 22658, 115, 7904, 97, 1030, 3372, 936, 1417, 3372, 115, 3493, 423, 533, 2813, 13422, 137, 9]\n",
      "think people find name search engines like google use common words especially single words like trouble heart may hard fans find online\n",
      "\n",
      "Output Summary:\n",
      "Word Ids: [25, 4, 23, 870, 27, 4]\n",
      "be the good critic to the\n"
     ]
    }
   ],
   "source": [
    "tests = [i for i in range(30)]\n",
    "\n",
    "checkpoint = \"checkpoints/best_model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    for t in tests:\n",
    "        try:\n",
    "            text = text_to_seq(texts[t])\n",
    "            #Multiply by batch_size to match the model's input parameters\n",
    "            answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                          summary_length: [np.random.randint(5,8)], \n",
    "                                          text_length: [len(text)]*batch_size,\n",
    "                                          keep_prob: 1.0})[0] \n",
    "\n",
    "            # Remove the padding from the tweet\n",
    "            pad = vocab_to_int[\"<PAD>\"]\n",
    "            # print('Original Text:', wikihow.text[random])\n",
    "            # print('Original summary:', wikihow.summary[random])#clean_summaries[random]\n",
    "            print('\\n==================== Example {} ===================='.format(t))\n",
    "            print('Original Text:')\n",
    "            print(texts[t])\n",
    "            print('\\nOriginal Summary:')\n",
    "            print(summaries[t])\n",
    "            print('\\nInput:')\n",
    "            print('Word Ids: {}'.format([i for i in text]))\n",
    "            print('{}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
    "            print('\\nOutput Summary:')\n",
    "            print('Word Ids: {}'.format([i for i in answer_logits if i != pad]))\n",
    "            print('{}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
