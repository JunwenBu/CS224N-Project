{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataprocessing for Pointer Generator Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate a CSV file with headers: content, title.\n",
    "2. Tokenize data.\n",
    "3. Process into .bin and vocab files.\n",
    "4. Lastly, split train.bin, val.bin and test.bin into chunks of 1000 examples per chunk.\n",
    "The generated bin files (the single or the chunked ones) can be used as input of Pointer-Generator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import hashlib\n",
    "import struct\n",
    "import subprocess\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wikihow = pd.read_csv(\"data/clean_wikihow_pointer.csv\")\n",
    "summaries = wikihow['summary'].tolist()\n",
    "texts = wikihow['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sell yourself first\n",
      "before doing anything else, stop and sum up yourself as an artist. now, think about how to translate that to an online profile. be it the few words, twitter allows you or an entire page of indulgence that your own website would allow you. bring out the most salient features of your creativity, your experience, your passion, and your reasons for painting. make it clear to readers why you are an artist who loves art, produces high quality art, and is a true champion of art. if you are not great with words, find a friend who can help you with this really important aspect of selling online – the establishment of your credibility and reliability.\n",
      "\n",
      "read the classics before 1600\n",
      "reading the classics is the very first thing you have to do to be well-read. if you want to build a solid foundation for your understanding of the books you read, then you cannot avoid some of the earliest plays, poems, and oral tales ever written down. remember that the novel did not really get popular until the 18th century, so you will not find novels on this list. without reading the poetry of homer or the plays of sophocles, you will not be able to call yourself well-read. here s a list to get you started:the epic of gilgamesh (unknown author) (18th – 17th century bce)the iliad and the odyssey by homer (850–750 bce, 8th century bce)\"the oresteia\" by aeschylus (458 bce)oedipus the king by sophocles (430 bce)medea by euripides (431 bce)aeneid by virgil (29–19 bce)one thousand and one nights (unknown author) (700–1500)beowulf (unknown author) (975-1025)the tale of genji by murasaki shikibu (11th century)the divine comedy by dante (1265–1321)the decameron by boccaccio (1349–53)the canterbury tales by chaucer (14th century)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contents = []\n",
    "titles = []\n",
    "\n",
    "# Here index '-1' to remove last ';' from raw text.\n",
    "for t, s in zip(texts, summaries):\n",
    "    if (not isinstance(t, float)) and (not isinstance(s, float)):\n",
    "        contents.append(t[:-1])\n",
    "        titles.append(s[:-1])\n",
    "\n",
    "for i in range(2):\n",
    "    print(titles[i])\n",
    "    print(contents[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1212012 1212012\n"
     ]
    }
   ],
   "source": [
    "print(len(contents), len(titles))\n",
    "# range of the dataset\n",
    "# train: data_size - train_end\n",
    "# val: data_size - train_end - val_end\n",
    "# val: train_end - val_end\n",
    "si, ei = 0, 504000\n",
    "train_end = 2000\n",
    "val_end = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>before doing anything else, stop and sum up yo...</td>\n",
       "      <td>sell yourself first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reading the classics is the very first thing y...</td>\n",
       "      <td>read the classics before 1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>depending on what scale you intend to sell you...</td>\n",
       "      <td>join online artist communities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get yourself out there as best as you can by a...</td>\n",
       "      <td>make yourself public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>given the hundreds of free blogging websites, ...</td>\n",
       "      <td>blog about your artwork</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  before doing anything else, stop and sum up yo...   \n",
       "1  reading the classics is the very first thing y...   \n",
       "2  depending on what scale you intend to sell you...   \n",
       "3  get yourself out there as best as you can by a...   \n",
       "4  given the hundreds of free blogging websites, ...   \n",
       "\n",
       "                            title  \n",
       "0             sell yourself first  \n",
       "1   read the classics before 1600  \n",
       "2  join online artist communities  \n",
       "3            make yourself public  \n",
       "4         blog about your artwork  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {'content': contents[si:ei], 'title': titles[si:ei]}\n",
    "df = pd.DataFrame(dic)\n",
    "df.to_csv('dataset.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code used to tokenize and generate bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_single_close_quote = u'\\u2019' # unicode\n",
    "dm_double_close_quote = u'\\u201d'\n",
    "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote, \")\"] # acceptable ways to end a sentence\n",
    "\n",
    "# We use these to separate the summary sentences in the .bin datafiles\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "all_train_urls = \"\"\n",
    "all_val_urls = \"\"\n",
    "all_test_urls = \"\"\n",
    "\n",
    "tokenized_stories_dir = \"wikihow_tokenized\" #location of folder to tokenize text\n",
    "finished_files_dir = \"wikihow_finished_files\" #final ouput\n",
    "chunks_dir = os.path.join(finished_files_dir, \"chunked\")\n",
    "\n",
    "VOCAB_SIZE = 200000\n",
    "CHUNK_SIZE = 1000 # num examples per chunk, for the chunked data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_file(set_name):\n",
    "  in_file = finished_files_dir + '/%s.bin' % set_name\n",
    "  reader = open(in_file, \"rb\")\n",
    "  chunk = 0\n",
    "  finished = False\n",
    "  while not finished:\n",
    "    chunk_fname = os.path.join(chunks_dir, '%s_%03d.bin' % (set_name, chunk)) # new chunk\n",
    "    with open(chunk_fname, 'wb') as writer:\n",
    "      for _ in range(CHUNK_SIZE):\n",
    "        len_bytes = reader.read(8)\n",
    "        if not len_bytes:\n",
    "          finished = True\n",
    "          break\n",
    "        str_len = struct.unpack('q', len_bytes)[0]\n",
    "        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "        writer.write(struct.pack('q', str_len))\n",
    "        writer.write(struct.pack('%ds' % str_len, example_str))\n",
    "      chunk += 1\n",
    "\n",
    "def chunk_all():\n",
    "  # Make a dir to hold the chunks\n",
    "  if not os.path.isdir(chunks_dir):\n",
    "    os.mkdir(chunks_dir)\n",
    "  # Chunk the data\n",
    "  for set_name in ['train', 'val', 'test']:\n",
    "    print (\"Splitting %s data into chunks...\" % set_name)\n",
    "    chunk_file(set_name)\n",
    "  print (\"Saved chunked data in %s\" % chunks_dir)\n",
    "\n",
    "def tokenize_stories(reviews, tokenized_stories_dir):\n",
    "  \"\"\"Maps a whole directory of .story files to a tokenized version using Stanford CoreNLP Tokenizer\"\"\"\n",
    "  for i, row in tqdm(reviews.iterrows(), total=reviews.shape[0]):\n",
    "        filename = str(i) + '.tok'\n",
    "        with open(os.path.join(tokenized_stories_dir, filename), 'w', encoding=\"utf-8\") as temp_file:\n",
    "            text = row[\"content\"]\n",
    "            tok = nltk.word_tokenize(text)\n",
    "            tok.append(\"@highlight\")\n",
    "            Summary = row[\"title\"]\n",
    "            tok.extend(nltk.word_tokenize(Summary))\n",
    "            list = tok.copy()\n",
    "\n",
    "            for i in tok:\n",
    "                if(i=='``' or i==\"''\" ):\n",
    "                    list.remove(i)\n",
    "            tok_string = \"\\n\".join(str(x) for x in list)\n",
    "            temp_file.write(tok_string)\n",
    "  print (\"Successfully finished tokenizing to %s .\\n\" % (tokenized_stories_dir))\n",
    "\n",
    "def fix_missing_period(line):\n",
    "  \"\"\"Adds a period to a line that is missing a period\"\"\"\n",
    "  if \"@highlight\" in line: return line\n",
    "  if line==\"\": return line\n",
    "  if line[-1] in END_TOKENS: return line\n",
    "  # print line[-1]\n",
    "  return line + \" .\"\n",
    "\n",
    "def read_text_file(text_file):\n",
    "  lines = []\n",
    "  with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "      lines.append(line.strip())\n",
    "  return lines\n",
    "\n",
    "def get_art_abs(story_file):\n",
    "  lines = read_text_file(story_file)\n",
    "\n",
    "  # Lowercase everything\n",
    "  lines = [line.lower() for line in lines]\n",
    "\n",
    "  # Put periods on the ends of lines that are missing them (this is a problem in the dataset because many image captions don't end in periods; consequently they end up in the body of the article as run-on sentences)\n",
    "  lines = [fix_missing_period(line) for line in lines]\n",
    "\n",
    "  # Separate out article and abstract sentences\n",
    "  article_lines = []\n",
    "  highlights = []\n",
    "  next_is_highlight = False\n",
    "  for idx,line in enumerate(lines):\n",
    "    if line == \"\":\n",
    "      continue # empty line\n",
    "    elif line.startswith(\"@highlight\"):\n",
    "      next_is_highlight = True\n",
    "    elif next_is_highlight:\n",
    "      highlights.append(line)\n",
    "    else:\n",
    "      article_lines.append(line)\n",
    "\n",
    "  # Make article into a single string\n",
    "  article = ' '.join(article_lines)\n",
    "\n",
    "  # Make abstract into a signle string, putting <s> and </s> tags around the sentences\n",
    "  abstract = ' '.join([\"%s %s %s\" % (SENTENCE_START, sent, SENTENCE_END) for sent in highlights])\n",
    "\n",
    "  return article, abstract\n",
    "\n",
    "def write_to_bin(file_names, out_file, makevocab=False):\n",
    "  \"\"\"Reads the tokenized .story files corresponding to the urls listed in the url_file and writes them to a out_file.\"\"\"\n",
    "  story_fnames = [str(s)+\".tok\" for s in file_names]\n",
    "  num_stories = len(story_fnames)\n",
    "\n",
    "  if makevocab:\n",
    "    vocab_counter = collections.Counter()\n",
    "\n",
    "  with open(out_file, 'wb') as writer:\n",
    "    for idx,s in enumerate(tqdm(story_fnames)):\n",
    "      # Look in the tokenized story dirs to find the .story file corresponding to this url\n",
    "      if os.path.isfile(os.path.join(tokenized_stories_dir, s)):\n",
    "        story_file = os.path.join(tokenized_stories_dir, s)\n",
    "      else:\n",
    "        print (\"Error: Couldn't find tokenized story file %s in either tokenized story directory %s. Was there an error during tokenization?\" % (s, tokenized_stories_dir))\n",
    "        # Check again if tokenized stories directories contain correct number of files\n",
    "        print (\"Checking that the tokenized stories directory %s contain correct number of files...\" % (tokenized_stories_dir))\n",
    "        \n",
    "      # Get the strings to write to .bin file\n",
    "      article, abstract = get_art_abs(story_file)\n",
    "\n",
    "      # Write to tf.Example\n",
    "      tf_example = example_pb2.Example()\n",
    "      tf_example.features.feature['article'].bytes_list.value.extend([article.encode('utf-8')])\n",
    "      tf_example.features.feature['abstract'].bytes_list.value.extend([abstract.encode('utf-8')])\n",
    "      tf_example_str = tf_example.SerializeToString()\n",
    "      str_len = len(tf_example_str)\n",
    "      writer.write(struct.pack('q', str_len))\n",
    "      writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "   \n",
    "      # Write the vocab to file, if applicable\n",
    "      if makevocab:\n",
    "        art_tokens = article.split(' ')\n",
    "        abs_tokens = abstract.split(' ')\n",
    "        abs_tokens = [t for t in abs_tokens if t not in [SENTENCE_START, SENTENCE_END]] # remove these tags from vocab\n",
    "        tokens = art_tokens + abs_tokens\n",
    "        tokens = [t.strip() for t in tokens] # strip\n",
    "        tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "        vocab_counter.update(tokens)\n",
    "  print (\"Finished writing file %s\\n\" % out_file)\n",
    "\n",
    "  # write vocab to file\n",
    "  if makevocab:\n",
    "    print (\"Writing vocab file...\")\n",
    "    with open(os.path.join(finished_files_dir, \"vocab\"), 'w', encoding=\"utf-8\") as writer:\n",
    "      for word, count in vocab_counter.most_common(VOCAB_SIZE):\n",
    "        writer.write(word + ' ' + str(count) + '\\n')\n",
    "    print (\"Finished writing vocab file\")\n",
    "\n",
    "def check_num_stories(stories_dir, num_expected):\n",
    "  num_stories = len(os.listdir(stories_dir))\n",
    "  if num_stories != num_expected:\n",
    "    raise Exception(\"stories directory %s contains %i files but should contain %i\" % (stories_dir, num_stories, num_expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 503999/503999 [42:20<00:00, 198.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully finished tokenizing to wikihow_tokenized .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stories_dir =  r\"C:\\Training\\cs224n\\proj\\sum\"\n",
    "# Create some new directories\n",
    "if not os.path.exists(tokenized_stories_dir): os.makedirs(tokenized_stories_dir)\n",
    "if not os.path.exists(finished_files_dir): os.makedirs(finished_files_dir)\n",
    "\n",
    "#data needed is in a csv format\n",
    "#containg 2 columbs (content , title)\n",
    "reviews_csv = stories_dir + \"\\dataset.csv\"\n",
    "reviews = pd.read_csv(reviews_csv)\n",
    "reviews = reviews.filter(['content', 'title'])\n",
    "reviews = reviews.dropna()\n",
    "reviews = reviews.reset_index(drop=True)\n",
    "\n",
    "# Run nltk tokenizer on both text and summary , outputting to tokenized stories directories\n",
    "tokenize_stories(reviews, tokenized_stories_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1225.76it/s]\n",
      " 11%|████████▍                                                                    | 110/1000 [00:00<00:00, 1091.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing file wikihow_finished_files\\test.bin\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1164.54it/s]\n",
      "  0%|                                                                                       | 0/501999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing file wikihow_finished_files\\val.bin\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 501999/501999 [08:03<00:00, 1038.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing file wikihow_finished_files\\train.bin\n",
      "\n",
      "Writing vocab file...\n",
      "Finished writing vocab file\n"
     ]
    }
   ],
   "source": [
    "#to get the length of your dataset\n",
    "num_expected_stories =reviews.shape[0]\n",
    "all_train_urls = range(0,num_expected_stories-train_end)\n",
    "all_val_urls = range(num_expected_stories-train_end, num_expected_stories-val_end)\n",
    "all_test_urls = range(num_expected_stories-val_end,num_expected_stories)\n",
    "\n",
    "# Read the tokenized stories, do a little postprocessing then write to bin files\n",
    "write_to_bin(all_test_urls, os.path.join(finished_files_dir, \"test.bin\"))\n",
    "write_to_bin(all_val_urls, os.path.join(finished_files_dir, \"val.bin\"))\n",
    "write_to_bin(all_train_urls, os.path.join(finished_files_dir, \"train.bin\"), makevocab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train data into chunks...\n",
      "Splitting val data into chunks...\n",
      "Splitting test data into chunks...\n",
      "Saved chunked data in wikihow_finished_files\\chunked\n"
     ]
    }
   ],
   "source": [
    "# Chunk the data. This splits each of train.bin, val.bin and test.bin \n",
    "#into smaller chunks, each containing e.g. 1000 examples, and saves them in finished_files/chunks\n",
    "chunk_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
