{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataprocessing for Pointer Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import hashlib\n",
    "import struct\n",
    "import subprocess\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sell yourself first</td>\n",
       "      <td>anything else stop sum artist think translate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>read the classics before 1600</td>\n",
       "      <td>reading classics first thing well read want bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>join online artist communities</td>\n",
       "      <td>depending scale intend sell art pieces may wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>make yourself public</td>\n",
       "      <td>get best advertising publish example pieces ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blog about your artwork</td>\n",
       "      <td>given hundreds free blogging websites lot choi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           summary  \\\n",
       "0             sell yourself first    \n",
       "1   read the classics before 1600    \n",
       "2  join online artist communities    \n",
       "3            make yourself public    \n",
       "4         blog about your artwork    \n",
       "\n",
       "                                                text  \n",
       "0  anything else stop sum artist think translate ...  \n",
       "1  reading classics first thing well read want bu...  \n",
       "2  depending scale intend sell art pieces may wan...  \n",
       "3  get best advertising publish example pieces ar...  \n",
       "4  given hundreds free blogging websites lot choi...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikihow = pd.read_csv(\"data/clean_wikihow.csv\")\n",
    "wikihow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1212030 1212030\n"
     ]
    }
   ],
   "source": [
    "summaries = wikihow['summary'].tolist()\n",
    "texts = wikihow['text'].tolist()\n",
    "print(len(summaries), len(texts))\n",
    "# range of the dataset\n",
    "si, ei = 0, 200000\n",
    "\n",
    "train_end = 2000\n",
    "val_end = 1000\n",
    "# all_train_urls = range(0, num_expected_cnn_stories - train_end)\n",
    "# all_val_urls = range(num_expected_cnn_stories - train_end, num_expected_cnn_stories - val_end)\n",
    "# all_test_urls = range(num_expected_cnn_stories - val_end, num_expected_cnn_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anything else stop sum artist think translate ...</td>\n",
       "      <td>sell yourself first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reading classics first thing well read want bu...</td>\n",
       "      <td>read the classics before 1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>depending scale intend sell art pieces may wan...</td>\n",
       "      <td>join online artist communities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get best advertising publish example pieces ar...</td>\n",
       "      <td>make yourself public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>given hundreds free blogging websites lot choi...</td>\n",
       "      <td>blog about your artwork</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  anything else stop sum artist think translate ...   \n",
       "1  reading classics first thing well read want bu...   \n",
       "2  depending scale intend sell art pieces may wan...   \n",
       "3  get best advertising publish example pieces ar...   \n",
       "4  given hundreds free blogging websites lot choi...   \n",
       "\n",
       "                             title  \n",
       "0             sell yourself first   \n",
       "1   read the classics before 1600   \n",
       "2  join online artist communities   \n",
       "3            make yourself public   \n",
       "4         blog about your artwork   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {'content': texts[si:ei], 'title': summaries[si:ei]}\n",
    "df = pd.DataFrame(dic)\n",
    "df.to_csv('dataset.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code used to tokenize and generate bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_single_close_quote = u'\\u2019' # unicode\n",
    "dm_double_close_quote = u'\\u201d'\n",
    "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote, \")\"] # acceptable ways to end a sentence\n",
    "\n",
    "# We use these to separate the summary sentences in the .bin datafiles\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "all_train_urls = \"\"\n",
    "all_val_urls = \"\"\n",
    "all_test_urls = \"\"\n",
    "\n",
    "cnn_tokenized_stories_dir = \"wikihow_tokenized\" #location of folder to tokenize text\n",
    "dm_tokenized_stories_dir = \"dm_stories_tokenized\" #not used\n",
    "finished_files_dir = \"wikihow_finished_files\" #final ouput\n",
    "chunks_dir = os.path.join(finished_files_dir, \"chunked\")\n",
    "\n",
    "VOCAB_SIZE = 200000\n",
    "CHUNK_SIZE = 1000 # num examples per chunk, for the chunked data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_file(set_name):\n",
    "  in_file = finished_files_dir + '/%s.bin' % set_name\n",
    "  reader = open(in_file, \"rb\")\n",
    "  chunk = 0\n",
    "  finished = False\n",
    "  while not finished:\n",
    "    chunk_fname = os.path.join(chunks_dir, '%s_%03d.bin' % (set_name, chunk)) # new chunk\n",
    "    with open(chunk_fname, 'wb') as writer:\n",
    "      for _ in range(CHUNK_SIZE):\n",
    "        len_bytes = reader.read(8)\n",
    "        if not len_bytes:\n",
    "          finished = True\n",
    "          break\n",
    "        str_len = struct.unpack('q', len_bytes)[0]\n",
    "        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "        writer.write(struct.pack('q', str_len))\n",
    "        writer.write(struct.pack('%ds' % str_len, example_str))\n",
    "      chunk += 1\n",
    "\n",
    "def chunk_all():\n",
    "  # Make a dir to hold the chunks\n",
    "  if not os.path.isdir(chunks_dir):\n",
    "    os.mkdir(chunks_dir)\n",
    "  # Chunk the data\n",
    "  for set_name in ['train', 'val', 'test']:\n",
    "    print (\"Splitting %s data into chunks...\" % set_name)\n",
    "    chunk_file(set_name)\n",
    "  print (\"Saved chunked data in %s\" % chunks_dir)\n",
    "\n",
    "def tokenize_stories(reviews, tokenized_stories_dir):\n",
    "  \"\"\"Maps a whole directory of .story files to a tokenized version using Stanford CoreNLP Tokenizer\"\"\"\n",
    "#  progress\\= ProgressBar.ProgressBar(len(reviews), fmt=ProgressBar.ProgressBar.FULL)\n",
    "\n",
    "  for i, row in reviews.iterrows():\n",
    "        filename = str(i) + '.tok'\n",
    "        with open(os.path.join(tokenized_stories_dir, filename), 'w', encoding=\"utf-8\") as temp_file:\n",
    "            text = row[\"content\"]\n",
    "            tok = nltk.word_tokenize(text)\n",
    "            tok.append(\"@highlight\")\n",
    "            Summary = row[\"title\"]\n",
    "            tok.extend(nltk.word_tokenize(Summary))\n",
    "            list = tok.copy()\n",
    "\n",
    "            for i in tok:\n",
    "                if(i=='``' or i==\"''\" ):\n",
    "                    list.remove(i)\n",
    "            tok_string = \"\\n\".join(str(x) for x in list)\n",
    "            temp_file.write(tok_string)\n",
    "  print (\"Successfully finished tokenizing to %s .\\n\" % (tokenized_stories_dir))\n",
    "\n",
    "def fix_missing_period(line):\n",
    "  \"\"\"Adds a period to a line that is missing a period\"\"\"\n",
    "  if \"@highlight\" in line: return line\n",
    "  if line==\"\": return line\n",
    "  if line[-1] in END_TOKENS: return line\n",
    "  # print line[-1]\n",
    "  return line + \" .\"\n",
    "\n",
    "def read_text_file(text_file):\n",
    "  lines = []\n",
    "  with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "      lines.append(line.strip())\n",
    "  return lines\n",
    "\n",
    "def get_art_abs(story_file):\n",
    "  lines = read_text_file(story_file)\n",
    "\n",
    "  # Lowercase everything\n",
    "  lines = [line.lower() for line in lines]\n",
    "\n",
    "  # Put periods on the ends of lines that are missing them (this is a problem in the dataset because many image captions don't end in periods; consequently they end up in the body of the article as run-on sentences)\n",
    "  lines = [fix_missing_period(line) for line in lines]\n",
    "\n",
    "  # Separate out article and abstract sentences\n",
    "  article_lines = []\n",
    "  highlights = []\n",
    "  next_is_highlight = False\n",
    "  for idx,line in enumerate(lines):\n",
    "    if line == \"\":\n",
    "      continue # empty line\n",
    "    elif line.startswith(\"@highlight\"):\n",
    "      next_is_highlight = True\n",
    "    elif next_is_highlight:\n",
    "      highlights.append(line)\n",
    "    else:\n",
    "      article_lines.append(line)\n",
    "\n",
    "  # Make article into a single string\n",
    "  article = ' '.join(article_lines)\n",
    "\n",
    "  # Make abstract into a signle string, putting <s> and </s> tags around the sentences\n",
    "  abstract = ' '.join([\"%s %s %s\" % (SENTENCE_START, sent, SENTENCE_END) for sent in highlights])\n",
    "\n",
    "  return article, abstract\n",
    "\n",
    "def write_to_bin(file_names, out_file, makevocab=False):\n",
    "  \"\"\"Reads the tokenized .story files corresponding to the urls listed in the url_file and writes them to a out_file.\"\"\"\n",
    " \n",
    "  story_fnames = [str(s)+\".tok\" for s in file_names]\n",
    "  num_stories = len(story_fnames)\n",
    "\n",
    "  if makevocab:\n",
    "    vocab_counter = collections.Counter()\n",
    "\n",
    "  with open(out_file, 'wb') as writer:\n",
    "    for idx,s in enumerate(story_fnames):\n",
    "      if idx % 1000 == 0:\n",
    "        print( \"Writing story %i of %i; %.2f percent done\" % (idx, num_stories, float(idx)*100.0/float(num_stories)))\n",
    "\n",
    "      # Look in the tokenized story dirs to find the .story file corresponding to this url\n",
    "      if os.path.isfile(os.path.join(cnn_tokenized_stories_dir, s)):\n",
    "        story_file = os.path.join(cnn_tokenized_stories_dir, s)\n",
    "      elif os.path.isfile(os.path.join(dm_tokenized_stories_dir, s)):\n",
    "        story_file = os.path.join(dm_tokenized_stories_dir, s)\n",
    "      else:\n",
    "        print (\"Error: Couldn't find tokenized story file %s in either tokenized story directories %s and %s. Was there an error during tokenization?\" % (s, cnn_tokenized_stories_dir, dm_tokenized_stories_dir))\n",
    "        # Check again if tokenized stories directories contain correct number of files\n",
    "        print (\"Checking that the tokenized stories directories %s and %s contain correct number of files...\" % (cnn_tokenized_stories_dir, dm_tokenized_stories_dir))\n",
    "        \n",
    "      # Get the strings to write to .bin file\n",
    "      article, abstract = get_art_abs(story_file)\n",
    "\n",
    "      # Write to tf.Example\n",
    "      tf_example = example_pb2.Example()\n",
    "      tf_example.features.feature['article'].bytes_list.value.extend([article.encode('utf-8')])\n",
    "      tf_example.features.feature['abstract'].bytes_list.value.extend([abstract.encode('utf-8')])\n",
    "      tf_example_str = tf_example.SerializeToString()\n",
    "      str_len = len(tf_example_str)\n",
    "      writer.write(struct.pack('q', str_len))\n",
    "      writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "   \n",
    "      # Write the vocab to file, if applicable\n",
    "      if makevocab:\n",
    "        art_tokens = article.split(' ')\n",
    "        abs_tokens = abstract.split(' ')\n",
    "        abs_tokens = [t for t in abs_tokens if t not in [SENTENCE_START, SENTENCE_END]] # remove these tags from vocab\n",
    "        tokens = art_tokens + abs_tokens\n",
    "        tokens = [t.strip() for t in tokens] # strip\n",
    "        tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "        vocab_counter.update(tokens)\n",
    "\n",
    "  print (\"Finished writing file %s\\n\" % out_file)\n",
    "\n",
    "  # write vocab to file\n",
    "  if makevocab:\n",
    "    print (\"Writing vocab file...\")\n",
    "    with open(os.path.join(finished_files_dir, \"vocab\"), 'w', encoding=\"utf-8\") as writer:\n",
    "      for word, count in vocab_counter.most_common(VOCAB_SIZE):\n",
    "        writer.write(word + ' ' + str(count) + '\\n')\n",
    "    print (\"Finished writing vocab file\")\n",
    "\n",
    "def check_num_stories(stories_dir, num_expected):\n",
    "  num_stories = len(os.listdir(stories_dir))\n",
    "  if num_stories != num_expected:\n",
    "    raise Exception(\"stories directory %s contains %i files but should contain %i\" % (stories_dir, num_stories, num_expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully finished tokenizing to wikihow_tokenized .\n",
      "\n",
      "Writing story 0 of 1000; 0.00 percent done\n",
      "Finished writing file wikihow_finished_files\\test.bin\n",
      "\n",
      "Writing story 0 of 1000; 0.00 percent done\n",
      "Finished writing file wikihow_finished_files\\val.bin\n",
      "\n",
      "Writing story 0 of 197994; 0.00 percent done\n",
      "Writing story 1000 of 197994; 0.51 percent done\n",
      "Writing story 2000 of 197994; 1.01 percent done\n",
      "Writing story 3000 of 197994; 1.52 percent done\n",
      "Writing story 4000 of 197994; 2.02 percent done\n",
      "Writing story 5000 of 197994; 2.53 percent done\n",
      "Writing story 6000 of 197994; 3.03 percent done\n",
      "Writing story 7000 of 197994; 3.54 percent done\n",
      "Writing story 8000 of 197994; 4.04 percent done\n",
      "Writing story 9000 of 197994; 4.55 percent done\n",
      "Writing story 10000 of 197994; 5.05 percent done\n",
      "Writing story 11000 of 197994; 5.56 percent done\n",
      "Writing story 12000 of 197994; 6.06 percent done\n",
      "Writing story 13000 of 197994; 6.57 percent done\n",
      "Writing story 14000 of 197994; 7.07 percent done\n",
      "Writing story 15000 of 197994; 7.58 percent done\n",
      "Writing story 16000 of 197994; 8.08 percent done\n",
      "Writing story 17000 of 197994; 8.59 percent done\n",
      "Writing story 18000 of 197994; 9.09 percent done\n",
      "Writing story 19000 of 197994; 9.60 percent done\n",
      "Writing story 20000 of 197994; 10.10 percent done\n",
      "Writing story 21000 of 197994; 10.61 percent done\n",
      "Writing story 22000 of 197994; 11.11 percent done\n",
      "Writing story 23000 of 197994; 11.62 percent done\n",
      "Writing story 24000 of 197994; 12.12 percent done\n",
      "Writing story 25000 of 197994; 12.63 percent done\n",
      "Writing story 26000 of 197994; 13.13 percent done\n",
      "Writing story 27000 of 197994; 13.64 percent done\n",
      "Writing story 28000 of 197994; 14.14 percent done\n",
      "Writing story 29000 of 197994; 14.65 percent done\n",
      "Writing story 30000 of 197994; 15.15 percent done\n",
      "Writing story 31000 of 197994; 15.66 percent done\n",
      "Writing story 32000 of 197994; 16.16 percent done\n",
      "Writing story 33000 of 197994; 16.67 percent done\n",
      "Writing story 34000 of 197994; 17.17 percent done\n",
      "Writing story 35000 of 197994; 17.68 percent done\n",
      "Writing story 36000 of 197994; 18.18 percent done\n",
      "Writing story 37000 of 197994; 18.69 percent done\n",
      "Writing story 38000 of 197994; 19.19 percent done\n",
      "Writing story 39000 of 197994; 19.70 percent done\n",
      "Writing story 40000 of 197994; 20.20 percent done\n",
      "Writing story 41000 of 197994; 20.71 percent done\n",
      "Writing story 42000 of 197994; 21.21 percent done\n",
      "Writing story 43000 of 197994; 21.72 percent done\n",
      "Writing story 44000 of 197994; 22.22 percent done\n",
      "Writing story 45000 of 197994; 22.73 percent done\n",
      "Writing story 46000 of 197994; 23.23 percent done\n",
      "Writing story 47000 of 197994; 23.74 percent done\n",
      "Writing story 48000 of 197994; 24.24 percent done\n",
      "Writing story 49000 of 197994; 24.75 percent done\n",
      "Writing story 50000 of 197994; 25.25 percent done\n",
      "Writing story 51000 of 197994; 25.76 percent done\n",
      "Writing story 52000 of 197994; 26.26 percent done\n",
      "Writing story 53000 of 197994; 26.77 percent done\n",
      "Writing story 54000 of 197994; 27.27 percent done\n",
      "Writing story 55000 of 197994; 27.78 percent done\n",
      "Writing story 56000 of 197994; 28.28 percent done\n",
      "Writing story 57000 of 197994; 28.79 percent done\n",
      "Writing story 58000 of 197994; 29.29 percent done\n",
      "Writing story 59000 of 197994; 29.80 percent done\n",
      "Writing story 60000 of 197994; 30.30 percent done\n",
      "Writing story 61000 of 197994; 30.81 percent done\n",
      "Writing story 62000 of 197994; 31.31 percent done\n",
      "Writing story 63000 of 197994; 31.82 percent done\n",
      "Writing story 64000 of 197994; 32.32 percent done\n",
      "Writing story 65000 of 197994; 32.83 percent done\n",
      "Writing story 66000 of 197994; 33.33 percent done\n",
      "Writing story 67000 of 197994; 33.84 percent done\n",
      "Writing story 68000 of 197994; 34.34 percent done\n",
      "Writing story 69000 of 197994; 34.85 percent done\n",
      "Writing story 70000 of 197994; 35.35 percent done\n",
      "Writing story 71000 of 197994; 35.86 percent done\n",
      "Writing story 72000 of 197994; 36.36 percent done\n",
      "Writing story 73000 of 197994; 36.87 percent done\n",
      "Writing story 74000 of 197994; 37.37 percent done\n",
      "Writing story 75000 of 197994; 37.88 percent done\n",
      "Writing story 76000 of 197994; 38.39 percent done\n",
      "Writing story 77000 of 197994; 38.89 percent done\n",
      "Writing story 78000 of 197994; 39.40 percent done\n",
      "Writing story 79000 of 197994; 39.90 percent done\n",
      "Writing story 80000 of 197994; 40.41 percent done\n",
      "Writing story 81000 of 197994; 40.91 percent done\n",
      "Writing story 82000 of 197994; 41.42 percent done\n",
      "Writing story 83000 of 197994; 41.92 percent done\n",
      "Writing story 84000 of 197994; 42.43 percent done\n",
      "Writing story 85000 of 197994; 42.93 percent done\n",
      "Writing story 86000 of 197994; 43.44 percent done\n",
      "Writing story 87000 of 197994; 43.94 percent done\n",
      "Writing story 88000 of 197994; 44.45 percent done\n",
      "Writing story 89000 of 197994; 44.95 percent done\n",
      "Writing story 90000 of 197994; 45.46 percent done\n",
      "Writing story 91000 of 197994; 45.96 percent done\n",
      "Writing story 92000 of 197994; 46.47 percent done\n",
      "Writing story 93000 of 197994; 46.97 percent done\n",
      "Writing story 94000 of 197994; 47.48 percent done\n",
      "Writing story 95000 of 197994; 47.98 percent done\n",
      "Writing story 96000 of 197994; 48.49 percent done\n",
      "Writing story 97000 of 197994; 48.99 percent done\n",
      "Writing story 98000 of 197994; 49.50 percent done\n",
      "Writing story 99000 of 197994; 50.00 percent done\n",
      "Writing story 100000 of 197994; 50.51 percent done\n",
      "Writing story 101000 of 197994; 51.01 percent done\n",
      "Writing story 102000 of 197994; 51.52 percent done\n",
      "Writing story 103000 of 197994; 52.02 percent done\n",
      "Writing story 104000 of 197994; 52.53 percent done\n",
      "Writing story 105000 of 197994; 53.03 percent done\n",
      "Writing story 106000 of 197994; 53.54 percent done\n",
      "Writing story 107000 of 197994; 54.04 percent done\n",
      "Writing story 108000 of 197994; 54.55 percent done\n",
      "Writing story 109000 of 197994; 55.05 percent done\n",
      "Writing story 110000 of 197994; 55.56 percent done\n",
      "Writing story 111000 of 197994; 56.06 percent done\n",
      "Writing story 112000 of 197994; 56.57 percent done\n",
      "Writing story 113000 of 197994; 57.07 percent done\n",
      "Writing story 114000 of 197994; 57.58 percent done\n",
      "Writing story 115000 of 197994; 58.08 percent done\n",
      "Writing story 116000 of 197994; 58.59 percent done\n",
      "Writing story 117000 of 197994; 59.09 percent done\n",
      "Writing story 118000 of 197994; 59.60 percent done\n",
      "Writing story 119000 of 197994; 60.10 percent done\n",
      "Writing story 120000 of 197994; 60.61 percent done\n",
      "Writing story 121000 of 197994; 61.11 percent done\n",
      "Writing story 122000 of 197994; 61.62 percent done\n",
      "Writing story 123000 of 197994; 62.12 percent done\n",
      "Writing story 124000 of 197994; 62.63 percent done\n",
      "Writing story 125000 of 197994; 63.13 percent done\n",
      "Writing story 126000 of 197994; 63.64 percent done\n",
      "Writing story 127000 of 197994; 64.14 percent done\n",
      "Writing story 128000 of 197994; 64.65 percent done\n",
      "Writing story 129000 of 197994; 65.15 percent done\n",
      "Writing story 130000 of 197994; 65.66 percent done\n",
      "Writing story 131000 of 197994; 66.16 percent done\n",
      "Writing story 132000 of 197994; 66.67 percent done\n",
      "Writing story 133000 of 197994; 67.17 percent done\n",
      "Writing story 134000 of 197994; 67.68 percent done\n",
      "Writing story 135000 of 197994; 68.18 percent done\n",
      "Writing story 136000 of 197994; 68.69 percent done\n",
      "Writing story 137000 of 197994; 69.19 percent done\n",
      "Writing story 138000 of 197994; 69.70 percent done\n",
      "Writing story 139000 of 197994; 70.20 percent done\n",
      "Writing story 140000 of 197994; 70.71 percent done\n",
      "Writing story 141000 of 197994; 71.21 percent done\n",
      "Writing story 142000 of 197994; 71.72 percent done\n",
      "Writing story 143000 of 197994; 72.22 percent done\n",
      "Writing story 144000 of 197994; 72.73 percent done\n",
      "Writing story 145000 of 197994; 73.23 percent done\n",
      "Writing story 146000 of 197994; 73.74 percent done\n",
      "Writing story 147000 of 197994; 74.24 percent done\n",
      "Writing story 148000 of 197994; 74.75 percent done\n",
      "Writing story 149000 of 197994; 75.25 percent done\n",
      "Writing story 150000 of 197994; 75.76 percent done\n",
      "Writing story 151000 of 197994; 76.26 percent done\n",
      "Writing story 152000 of 197994; 76.77 percent done\n",
      "Writing story 153000 of 197994; 77.28 percent done\n",
      "Writing story 154000 of 197994; 77.78 percent done\n",
      "Writing story 155000 of 197994; 78.29 percent done\n",
      "Writing story 156000 of 197994; 78.79 percent done\n",
      "Writing story 157000 of 197994; 79.30 percent done\n",
      "Writing story 158000 of 197994; 79.80 percent done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing story 159000 of 197994; 80.31 percent done\n",
      "Writing story 160000 of 197994; 80.81 percent done\n",
      "Writing story 161000 of 197994; 81.32 percent done\n",
      "Writing story 162000 of 197994; 81.82 percent done\n",
      "Writing story 163000 of 197994; 82.33 percent done\n",
      "Writing story 164000 of 197994; 82.83 percent done\n",
      "Writing story 165000 of 197994; 83.34 percent done\n",
      "Writing story 166000 of 197994; 83.84 percent done\n",
      "Writing story 167000 of 197994; 84.35 percent done\n",
      "Writing story 168000 of 197994; 84.85 percent done\n",
      "Writing story 169000 of 197994; 85.36 percent done\n",
      "Writing story 170000 of 197994; 85.86 percent done\n",
      "Writing story 171000 of 197994; 86.37 percent done\n",
      "Writing story 172000 of 197994; 86.87 percent done\n",
      "Writing story 173000 of 197994; 87.38 percent done\n",
      "Writing story 174000 of 197994; 87.88 percent done\n",
      "Writing story 175000 of 197994; 88.39 percent done\n",
      "Writing story 176000 of 197994; 88.89 percent done\n",
      "Writing story 177000 of 197994; 89.40 percent done\n",
      "Writing story 178000 of 197994; 89.90 percent done\n",
      "Writing story 179000 of 197994; 90.41 percent done\n",
      "Writing story 180000 of 197994; 90.91 percent done\n",
      "Writing story 181000 of 197994; 91.42 percent done\n",
      "Writing story 182000 of 197994; 91.92 percent done\n",
      "Writing story 183000 of 197994; 92.43 percent done\n",
      "Writing story 184000 of 197994; 92.93 percent done\n",
      "Writing story 185000 of 197994; 93.44 percent done\n",
      "Writing story 186000 of 197994; 93.94 percent done\n",
      "Writing story 187000 of 197994; 94.45 percent done\n",
      "Writing story 188000 of 197994; 94.95 percent done\n",
      "Writing story 189000 of 197994; 95.46 percent done\n",
      "Writing story 190000 of 197994; 95.96 percent done\n",
      "Writing story 191000 of 197994; 96.47 percent done\n",
      "Writing story 192000 of 197994; 96.97 percent done\n",
      "Writing story 193000 of 197994; 97.48 percent done\n",
      "Writing story 194000 of 197994; 97.98 percent done\n",
      "Writing story 195000 of 197994; 98.49 percent done\n",
      "Writing story 196000 of 197994; 98.99 percent done\n",
      "Writing story 197000 of 197994; 99.50 percent done\n",
      "Finished writing file wikihow_finished_files\\train.bin\n",
      "\n",
      "Writing vocab file...\n",
      "Finished writing vocab file\n",
      "Splitting train data into chunks...\n",
      "Splitting val data into chunks...\n",
      "Splitting test data into chunks...\n",
      "Saved chunked data in wikihow_finished_files\\chunked\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  #main directory\n",
    "  cnn_stories_dir =  r\"C:\\Training\\cs224n\\proj\\sum\"\n",
    "\n",
    "  # Create some new directories\n",
    "  if not os.path.exists(cnn_tokenized_stories_dir): os.makedirs(cnn_tokenized_stories_dir)\n",
    "  if not os.path.exists(finished_files_dir): os.makedirs(finished_files_dir)\n",
    "\n",
    "  #data needed is in a csv format\n",
    "  #containg 2 columbs (content , title)\n",
    "  reviews_csv =cnn_stories_dir + \"\\dataset.csv\"\n",
    "  reviews = pd.read_csv(reviews_csv)\n",
    "  reviews = reviews.filter(['content', 'title'])\n",
    "  reviews = reviews.dropna()\n",
    "  reviews = reviews.reset_index(drop=True)\n",
    "  reviews.head()\n",
    "\n",
    "  # Run nltk tokenizer on both text and summary , outputting to tokenized stories directories\n",
    "  tokenize_stories(reviews, cnn_tokenized_stories_dir)\n",
    "\n",
    "  #to get the length of your dataset\n",
    "  num_expected_cnn_stories =reviews.shape[0]\n",
    "\n",
    "  all_train_urls = range(0,num_expected_cnn_stories-train_end)\n",
    "  all_val_urls = range(num_expected_cnn_stories-train_end, num_expected_cnn_stories-val_end)\n",
    "  all_test_urls = range(num_expected_cnn_stories-val_end,num_expected_cnn_stories)\n",
    "\n",
    "  # Read the tokenized stories, do a little postprocessing then write to bin files\n",
    "  write_to_bin(all_test_urls, os.path.join(finished_files_dir, \"test.bin\"))\n",
    "  write_to_bin(all_val_urls, os.path.join(finished_files_dir, \"val.bin\"))\n",
    "  write_to_bin(all_train_urls, os.path.join(finished_files_dir, \"train.bin\"), makevocab=True)\n",
    "\n",
    "  # Chunk the data. This splits each of train.bin, val.bin and test.bin into smaller chunks, each containing e.g. 1000 examples, and saves them in finished_files/chunks\n",
    "  chunk_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
